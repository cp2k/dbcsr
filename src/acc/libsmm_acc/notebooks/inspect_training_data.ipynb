{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# `libcusmm`: Explore the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows you to explore the training data collected from autotuning before proceeding to training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys, os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from nb_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read training data from autotuning folders \n",
    "\n",
    "Read from files of form `tune_*x*x*/raw_training_data_*x*x*_algo.csv`. \n",
    "If you want to read from aggregated Parquet files (recommended), skip to lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to autotuning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the path to the autotuning data:\n",
    "- You can use the bash cell below to navigate your filetree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -ad AUTOTUNING_DATA_PATH/tune_*x*x*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, copy what you've replaced `AUTOTUNING_DATA_PATH` with in the Python variable `autotuning_data_path` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotuning_data_path = '' # may not recognize '~', please provide an absolute path:\n",
    "check_autotuning_data_path(autotuning_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set options\n",
    "\n",
    "Set the following options appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_read = 100       # How many / which data folders to read. Options: \n",
    "                    # - 'all': reads from all available data folders. \n",
    "                    #   Beware, this might result in memory errors if large amounts of data are made available\n",
    "                    # - a number: reads this number of data folders (e.g. 100)\n",
    "                    # - a regex: reads the data folders with matching regex (e.g. tune_4x*x*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = get_algorithm_to_explore('all')   # algorithms to explore. Options: all, tiny, small, medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of folders to read\n",
    "folders_to_read = get_folders_to_read(to_read, autotuning_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files_to_read, derived_files_to_read = get_files_to_read(folders_to_read, algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_f = len(files_to_read)\n",
    "data_raw = dd.read_csv(raw_files_to_read, dtype={}).set_index(\"Unnamed: 0\")\n",
    "data_derived = dd.read_csv(derived_files_to_read, dtype={}).set_index(\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read training data from Parquet files\n",
    "\n",
    "Read from files of form `training_data_algorithm.parquet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to autotuning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the path to the autotuning data:\n",
    "- You can use the bash cell below to navigate your filetree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -ad AUTOTUNING_DATA_PATH/*.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then, copy what you've replaced `AUTOTUNING_DATA_PATH` with in the Python variable `training_data_path` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = '../tune_dataset_V100/' # may not recognize '~', please provide an absolute path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = \"small\" # algorithm to explore. Options: tiny, small, medium, largeDB1, largeDB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_data_file = os.path.join(training_data_path, \"training_data_\" + algorithm + \".parquet\")\n",
    "data = dd.read_parquet(parquet_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data inspection\n",
    "\n",
    "### Data head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(data.columns.values), page_width):\n",
    "    display(data.iloc[:,i:i+page_width].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data size        :', sys.getsizeof(data)/10**6, 'MB')\n",
    "print('Number of columns:', len(data.columns.values))\n",
    "print('Number of rows   : {:,}'.format(len(data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i in range(0, len(data.columns.values), page_width):\n",
    "#    display(data.iloc[:,i:i+page_width].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print('Number of columns:', len(data.columns), '\\nNumber of rows:', len(data.index), '\\n')\n",
    "for col in data.columns: \n",
    "    print('{:<40} {}'.format(col, data[col].dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature categories\n",
    "mnk = ['m', 'n', 'k']\n",
    "kernel_pars = ['algorithm', 'threads_per_blk', 'grouping', 'minblocks',\n",
    "               'tile_m', 'tile_n', 'w', 'v', 'nbytes_smem', 'nbytes_cmem', 'regs_per_thread']\n",
    "kernel_pars = list(set(kernel_pars) & set(data.columns.values))\n",
    "perf =  ['perf (Gflop/s)', 'perf_scaled']\n",
    "common = ['Gflops', 'mxnxk', 'size_a', 'size_b', 'size_c', 'nblks', \n",
    "          'warps_per_blk', 'nwarps', 'sm_desired', 'nthreads', 'ru_param_stack_unroll_factor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "Features in the left-most column correspond to \"raw\" parameters\n",
    "* **green** kernel parameters \n",
    "* **grey** GPU card properties (taken from Nvidia/AMD documentation) \n",
    "* **pink** autotuning parameters (taken from DBCSR codebase) \n",
    "\n",
    "Other features correspond to derived parameters, computed from the \"raw\" parameters\n",
    "* **yellow** matrix sizes\n",
    "* **light grey** launch parameters\n",
    "* **blue** and **purple** estimations of resource usages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parameters dependency graph](libsmm_acc_predictive_modeling_features.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 300000     # do not perform very long operations on row counts above this threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_to_profile = data\n",
    "n_rows_data = len(data)\n",
    "if n_rows_data > thresh:  # if it is a very large dataframe, perform op on subsampled rows\n",
    "    data_to_profile = data.sample(frac = thresh / n_rows_data)\n",
    "\n",
    "import pandas_profiling \n",
    "pandas_profiling.ProfileReport(data_to_profile.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Series from Dask to Pandas\n",
    "data_mxnxk = data['mxnxk'].compute()\n",
    "data_perf = data['perf (Gflop/s)'].compute()\n",
    "data_perf_scaled = data['perf_scaled'].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(data_mxnxk, data_perf, '.', markersize=1)\n",
    "plt.xlabel('Training (m, n, k) triplets (in order of increasing m*n*k)')\n",
    "plt.ylabel('Performance [Gflops]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization (scaled performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_mxnxk, data_perf_scaled, '.', markersize=1)\n",
    "plt.xlabel('Training (m, n, k) triplets (in order of increasing m*n*k)')\n",
    "plt.ylabel('Performance scaled (overall)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose (m, n, k) triplet\n",
    "m_plot, n_plot, k_plot = (4, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mnk = data[data['m'] == m_plot][ \n",
    "                data['n'] == n_plot][ \n",
    "                data['k'] == k_plot].compute()\n",
    "data_mnk.sort_values(by='perf (Gflop/s)', ascending=True, inplace=True)\n",
    "plt.plot(data_mnk['perf (Gflop/s)'].values)\n",
    "plt.xlabel('parameter set')\n",
    "plt.ylabel('perf (Gflop/s)')\n",
    "plt.title('Performance profile for kernel ' + str(m_plot) + 'x'+ str(n_plot) + 'x'+ str(k_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms with Bokeh\n",
    "from bokeh.plotting import figure \n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "\n",
    "# Create histogram\n",
    "num_bins = 100 \n",
    "hist, edges = np.histogram(data_mnk['perf (Gflop/s)'], bins=num_bins)\n",
    "df_hist = pd.DataFrame({'hist': hist, 'left': edges[:-1], 'right': edges[1:]})\n",
    "source = ColumnDataSource(df_hist)\n",
    "\n",
    "# Create tool \n",
    "hover = HoverTool(tooltips=[('# occurences', '@hist'), ('low', '@left'), ('high', '@right')])\n",
    "\n",
    "# Create the figure\n",
    "p = figure(plot_width=800, plot_height=800, title=\"Performance histogram\",\n",
    "           toolbar_location=None, tools=\"\")\n",
    "p.xgrid.grid_line_color = None\n",
    "p.xaxis.axis_label = \"Performance (GFlop/s)\"\n",
    "p.xaxis.major_label_orientation = 1.2\n",
    "p.yaxis.axis_label = \"# occurrences\"\n",
    "p.quad(source=source, bottom=0, top='hist', left='left', right='right', fill_color='blue')\n",
    "p.add_tools(hover)\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms with Bokeh\n",
    "from bokeh.plotting import figure \n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "\n",
    "# Create histogram\n",
    "num_bins = 100 \n",
    "hist, edges = np.histogram(data_mnk['perf_scaled'], bins=num_bins)\n",
    "df_hist = pd.DataFrame({'hist': hist, 'left': edges[:-1], 'right': edges[1:]})\n",
    "source = ColumnDataSource(df_hist)\n",
    "\n",
    "# Create tool \n",
    "hover = HoverTool(tooltips=[('# occurences', '@hist'), ('low', '@left'), ('high', '@right')])\n",
    "\n",
    "# Create the figure\n",
    "p = figure(plot_width=800, plot_height=800, title=\"Performance histogram\",\n",
    "           toolbar_location=None, tools=\"\")\n",
    "p.xgrid.grid_line_color = None\n",
    "p.xaxis.axis_label = \"Performance scaled\"\n",
    "p.xaxis.major_label_orientation = 1.2\n",
    "p.yaxis.axis_label = \"# occurrences\"\n",
    "p.quad(source=source, bottom=0, top='hist', left='left', right='right', fill_color='blue')\n",
    "p.add_tools(hover)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top slices of perf. distribution\n",
    "pars_autotuning_top = {\n",
    "    5: list(), \n",
    "    2: list(), \n",
    "    1: list(), \n",
    "    0.5: list()\n",
    "}\n",
    "max_perf = float(data_mnk['perf (Gflop/s)'].max())\n",
    "max_perf_idx = data_mnk['perf (Gflop/s)'].idxmax()\n",
    "max_perf_row = data_mnk.loc[max_perf_idx]\n",
    "max_perf_cond = max_perf_row[mnk + kernel_pars + ['perf (Gflop/s)']]\n",
    "\n",
    "print('Maximally performing parameter set:')\n",
    "display(max_perf_cond)\n",
    "for perc in pars_autotuning_top.keys():\n",
    "    lim = max_perf - max_perf*perc/100\n",
    "    blob = data_mnk.loc[data_mnk['perf (Gflop/s)'] >= lim]\n",
    "    print('\\ntop', perc, '%')\n",
    "    display(blob[kernel_pars + ['perf (Gflop/s)']].describe())\n",
    "    pars_autotuning_top[perc].append(blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairplot = data\n",
    "n_rows_data = len(data)\n",
    "if n_rows_data > thresh:  # if it is a very large dataframe, perform op on subsampled rows\n",
    "    data_pairplot = data.sample(frac = thresh / n_rows_data)\n",
    "\n",
    "sns.pairplot(data_pairplot[mnk + kernel_pars + perf].compute().dropna())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
