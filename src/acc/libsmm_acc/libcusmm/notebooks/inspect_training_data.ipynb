{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# `libcusmm` explore training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows you to explore the training data collected from autotuning before proceeding to training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys, os, json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to autotuning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the path to the autotuning data:\n",
    "- You can use the bash cell below to navigate your filetree. \n",
    "- Then, copy what you've replaced `AUTOTUNING_DATA_PATH` with in the Python variable `autotuning_data_path` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -ad AUTOTUNING_DATA_PATH/tune_dataset/tune_*x*x*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -ad ~/scratch/tune_dataset/tune_*x*x*/ | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotuning_data_path = '/users/alicej/scratch/tune_dataset'  # may not recognize '~', provide absolute complete paths\n",
    "assert os.path.exists(autotuning_data_path)\n",
    "assert len(os.listdir(autotuning_data_path)) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set options\n",
    "\n",
    "Set the following options appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_read = 100       # How many / which data folders to read. Options: \n",
    "                    # - 'all': reads from all available data folders. \n",
    "                    #   Beware, this might result in memory errors if large amounts of data are made available\n",
    "                    # - a number: reads this number of data folders (e.g. 100)\n",
    "                    # - a regex: reads the data folders with matching regex (e.g. tune_4x*x*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_folder_pattern = re.compile('tune_(\\d+)x(\\d+)x(\\d+)$')\n",
    "\n",
    "# Get the list of folders to read\n",
    "if to_read == 'all':\n",
    "    folders_to_read = [os.path.join(autotuning_data_path, f) for f in os.listdir(autotuning_data_path) \n",
    "                       if kernel_folder_pattern.match(f) is not None]\n",
    "elif isinstance(to_read, int):\n",
    "    folders_to_read = [os.path.join(autotuning_data_path, f) for f in os.listdir(autotuning_data_path) \n",
    "                       if kernel_folder_pattern.match(f) is not None]\n",
    "    folders_to_read = folders_to_read[:to_read]\n",
    "elif isinstance(to_read, str):\n",
    "    to_read = re.compile(to_read)\n",
    "    folders_to_read = [os.path.join(autotuning_data_path, f) for f in os.listdir(autotuning_data_path) \n",
    "                       if to_read.match(f) is not None]    \n",
    "else: \n",
    "    assert False, \"Cannot recognize option: \" + to_read\n",
    "\n",
    "num_folders_to_read = len(folders_to_read)\n",
    "assert num_folders_to_read > 0\n",
    "print(\"Data folders to be read from (total: {:,})\\n\".format(num_folders_to_read))\n",
    "[print(f) for f in folders_to_read]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'all'   # algorithms to explore. Options: all, tiny, small, medium\n",
    "                    #-------------------------------------------------------------------\n",
    "algo_to_read = [algorithm] if algorithm != 'all' else ['tiny', 'small', 'medium', 'largeDB1', 'largeDB2']\n",
    "print(\"Algorithm(s) to explore:\")\n",
    "[print(a) for a in algo_to_read]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 300000     # do not perform very long operations on row counts above this threshold\n",
    "                    #-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_list = list()\n",
    "kernel_folder_pattern = re.compile('tune_(\\d+x\\d+x\\d+)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, kernel_folder in enumerate(folders_to_read):\n",
    "    print('\\nfrom {}, read                                  ({}/{:,})'.format(kernel_folder, i+1, num_folders_to_read))\n",
    "        \n",
    "    for name_algo in algo_to_read: \n",
    "\n",
    "        mnk_string = kernel_folder_pattern.search(kernel_folder).groups()[0]\n",
    "        raw_file_base = 'raw_training_data_' + mnk_string + '_' + name_algo + '.csv'\n",
    "        raw_file = os.path.join(kernel_folder, raw_file_base)\n",
    "        derived_file_base = 'training_data_' + mnk_string + '_' + name_algo + '.csv'\n",
    "        derived_file = os.path.join(kernel_folder, derived_file_base) \n",
    "\n",
    "        if os.path.exists(raw_file) and os.path.exists(derived_file):\n",
    "\n",
    "            # Read raw parameters file\n",
    "            raw = pd.read_csv(raw_file, index_col=0)\n",
    "            raw['algorithm'] = np.array([name_algo]*len(raw.index.values))\n",
    "            print('\\t{:50} number of lines: {:>8,}'.format(raw_file_base, len(raw.index.values)))\n",
    "\n",
    "            # Read derived parameters file\n",
    "            derived = pd.read_csv(derived_file, index_col=0)\n",
    "            print('\\t{:50} number of lines: {:>8,}'.format(derived_file_base, len(raw.index.values)))\n",
    "            dataframe_list.append(pd.concat([raw, derived], axis=1))\n",
    "\n",
    "        else: \n",
    "            \n",
    "            if not os.path.exists(raw_file):\n",
    "                print('\\t...{:50} no file'.format(raw_file_base))\n",
    "            if not os.path.exists(derived_file):\n",
    "                print('\\t...{:50} no file'.format(derived_file_base))\n",
    "            \n",
    "print('Read all csv files, merging dataframes')\n",
    "data = pd.concat(dataframe_list, ignore_index=True)\n",
    "data.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page_width = 5 # columns per output line\n",
    "for i in range(0, len(data.columns.values), page_width):\n",
    "    display(data.iloc[:,i:i+page_width].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print('Data size        :', sys.getsizeof(data)/10**6, 'MB')\n",
    "print('Number of columns:', len(data.columns.values))\n",
    "print('Number of rows   : {:,}'.format(len(data.index.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(data.columns.values), page_width):\n",
    "    display(data.iloc[:,i:i+page_width].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of columns:', len(data.columns), '\\nNumber of rows:', len(data.index.values), '\\n')\n",
    "for col in data.columns: \n",
    "    print('{:<40} {}'.format(col, data[col].dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature categories\n",
    "mnk = ['m', 'n', 'k']\n",
    "kernel_pars = ['algorithm', 'threads_per_blk', 'grouping', 'minblocks',\n",
    "               'tile_m', 'tile_n', 'w', 'v', 'nbytes_smem', 'nbytes_cmem', 'regs_per_thread']\n",
    "kernel_pars = list(set(kernel_pars) & set(data.columns.values))\n",
    "perf =  ['perf (Gflop/s)', 'perf_squared', 'perf_scaled', 'perf_scaled_by_algo']\n",
    "common = ['Gflops', 'mxnxk', 'size_a', 'size_b', 'size_c', 'nblks', \n",
    "          'warps_per_blk', 'nwarps', 'sm_desired', 'nthreads', 'ru_param_stack_unroll_factor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "other_columns = list(set(data.columns.values) - set(kernel_pars + mnk + perf + common))\n",
    "irow = 0#random.sample(range(len(data.index.values)), 1)[0]\n",
    "row = data.iloc[irow,:]\n",
    "print(row[mnk], '\\n')\n",
    "print(row[kernel_pars], '\\n')\n",
    "print(row[perf], '\\n')\n",
    "print(row[common], '\\n')\n",
    "print(row[other_columns], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "Features in the left-most column correspond to \"raw\" parameters\n",
    "* **green** kernel parameters \n",
    "* **grey** CUDA card properties (taken from CUDA documentation) \n",
    "* **pink** autotuning parameters (taken from DBCSR codebase) \n",
    "\n",
    "Other features correspond to derived parameters, computed from the \"raw\" parameters\n",
    "* **yellow** matrix sizes\n",
    "* **light grey** launch parameters\n",
    "* **blue** and **purple** estimations of resource usages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parameters dependency graph](libcusmm_parameters_and_memory.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_profiling = data\n",
    "if len(data.index.values) > thresh:  # if it is a very large dataframe, perform op on subsampled rows \n",
    "    sampled_rows = random.sample(data_profiling.index.values.tolist(), thresh)\n",
    "    data_profiling = data.iloc[sampled_rows,:]\n",
    "\n",
    "import pandas_profiling \n",
    "pandas_profiling.ProfileReport(data_profiling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(data['mxnxk'], data['perf (Gflop/s)'], '.', markersize=1)\n",
    "plt.xlabel('Training (m, n, k) triplets (in order of increasing m*n*k)')\n",
    "plt.ylabel('Performance [Gflops]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(data['mxnxk'], data['perf_squared'], '.', markersize=1)\n",
    "plt.xlabel('Training (m, n, k) triplets (in order of increasing m*n*k)')\n",
    "plt.ylabel('Performance squared [Gflops]^2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization (scaled performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['mxnxk'], data['perf_scaled'], '.', markersize=1)\n",
    "plt.xlabel('Training (m, n, k) triplets (in order of increasing m*n*k)')\n",
    "plt.ylabel('Performance scaled (overall)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algos = np.unique(data['algorithm'])\n",
    "for algo in algos:\n",
    "    algo_data = data[data['algorithm'] == algo]\n",
    "    plt.figure()\n",
    "    plt.semilogx(algo_data['mxnxk'], algo_data['perf_scaled_by_algo'], '.', markersize=1)\n",
    "    plt.xlabel('Training (m, n, k) triplets (in order of increasing m*n*k)')\n",
    "    plt.ylabel('Performance scaled (by algorithm)')\n",
    "    plt.title(algo)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose (m, n, k) triplet\n",
    "m_plot, n_plot, k_plot = (4, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mnk = data[data['m'] == m_plot][ \n",
    "                data['n'] == n_plot][ \n",
    "                data['k'] == k_plot]\n",
    "data_mnk.sort_values(by='perf (Gflop/s)', ascending=True, inplace=True)\n",
    "plt.plot(data_mnk['perf (Gflop/s)'].values)\n",
    "plt.xlabel('parameter set')\n",
    "plt.ylabel('perf (Gflop/s)')\n",
    "plt.title('Performance profile for kernel ' + str(m_plot) + 'x'+ str(n_plot) + 'x'+ str(k_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms with Bokeh\n",
    "from bokeh.plotting import figure \n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "\n",
    "# Create histogram\n",
    "num_bins = 100 \n",
    "hist, edges = np.histogram(data_mnk['perf (Gflop/s)'], bins=num_bins)\n",
    "df_hist = pd.DataFrame({'hist': hist, 'left': edges[:-1], 'right': edges[1:]})\n",
    "source = ColumnDataSource(df_hist)\n",
    "\n",
    "# Create tool \n",
    "hover = HoverTool(tooltips=[('# occurences', '@hist'), ('low', '@left'), ('high', '@right')])\n",
    "\n",
    "# Create the figure\n",
    "p = figure(plot_width=800, plot_height=800, title=\"Performance histogram\",\n",
    "           toolbar_location=None, tools=\"\")\n",
    "p.xgrid.grid_line_color = None\n",
    "p.xaxis.axis_label = \"Performance (GFlop/s)\"\n",
    "p.xaxis.major_label_orientation = 1.2\n",
    "p.yaxis.axis_label = \"# occurrences\"\n",
    "p.quad(source=source, bottom=0, top='hist', left='left', right='right', fill_color='blue')\n",
    "p.add_tools(hover)\n",
    "show(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms with Bokeh\n",
    "from bokeh.plotting import figure \n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "\n",
    "# Create histogram\n",
    "num_bins = 100 \n",
    "hist, edges = np.histogram(data_mnk['perf_scaled'], bins=num_bins)\n",
    "df_hist = pd.DataFrame({'hist': hist, 'left': edges[:-1], 'right': edges[1:]})\n",
    "source = ColumnDataSource(df_hist)\n",
    "\n",
    "# Create tool \n",
    "hover = HoverTool(tooltips=[('# occurences', '@hist'), ('low', '@left'), ('high', '@right')])\n",
    "\n",
    "# Create the figure\n",
    "p = figure(plot_width=800, plot_height=800, title=\"Performance histogram\",\n",
    "           toolbar_location=None, tools=\"\")\n",
    "p.xgrid.grid_line_color = None\n",
    "p.xaxis.axis_label = \"Performance scaled\"\n",
    "p.xaxis.major_label_orientation = 1.2\n",
    "p.yaxis.axis_label = \"# occurrences\"\n",
    "p.quad(source=source, bottom=0, top='hist', left='left', right='right', fill_color='blue')\n",
    "p.add_tools(hover)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top slices of perf. distribution\n",
    "pars_autotuning_top = {\n",
    "    5: list(), \n",
    "    2: list(), \n",
    "    1: list(), \n",
    "    0.5: list()\n",
    "}\n",
    "max_perf = float(data_mnk['perf (Gflop/s)'].max())\n",
    "max_perf_idx = data_mnk['perf (Gflop/s)'].idxmax()\n",
    "max_perf_row = data_mnk.loc[max_perf_idx]\n",
    "max_perf_cond = max_perf_row[mnk + kernel_pars + ['perf (Gflop/s)']]\n",
    "\n",
    "print('Maximally performing parameter set:')\n",
    "display(max_perf_cond)\n",
    "for perc in pars_autotuning_top.keys():\n",
    "    lim = max_perf - max_perf*perc/100\n",
    "    blob = data_mnk.loc[data_mnk['perf (Gflop/s)'] >= lim]\n",
    "    print('\\ntop', perc, '%')\n",
    "    display(blob[kernel_pars + ['perf (Gflop/s)']].describe())\n",
    "    pars_autotuning_top[perc].append(blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairplot = data\n",
    "if len(data.index.values) > thresh:  # if it is a very large dataframe, perform op on subsampled rows \n",
    "    sampled_rows = random.sample(data_pairplot.index.values.tolist(), thresh)\n",
    "    data_pairplot = data.iloc[sampled_rows,:]\n",
    "\n",
    "sns.pairplot(data_pairplot[mnk + kernel_pars + perf].dropna())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
