<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
   
   <meta name="description" content="">
    
    <meta name="author" content="DBCSR Authors" >
    <link rel="icon" href="../../../../../favicon.png">

    <title>Autotuning Framework &ndash; DBCSR</title>

    <link href="../../../../../css/bootstrap.min.css" rel="stylesheet">
    <link href="../../../../../css/pygments.css" rel="stylesheet">
    <link href="../../../../../css/font-awesome.min.css" rel="stylesheet">
    <link href="../../../../../css/local.css" rel="stylesheet">
    
    

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    
    <script src="../../../../../js/jquery-2.1.3.min.js"></script>
    <script src="../../../../../js/svg-pan-zoom.min.js"></script>

  </head>

  <body>

    <!-- Fixed navbar -->
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="../../../../../index.html">DBCSR <small>2.2.0-rc0-36-g182c40574d</small></a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
        
            <li><a href='../../../../../page/index.html'>Guide</a></li>
      
            <li class="dropdown hidden-xs visible-sm visible-md hidden-lg">
              <a href="#" class="dropdown-toggle"
              data-toggle="dropdown" role="button"
              aria-haspopup="true"
     aria-expanded="false">Contents <span class="caret"></span></a>
        <ul class="dropdown-menu">
          
              
            <li><a href="../../../../../lists/files.html">Source Files</a></li>
        
        
        
            <li><a href="../../../../../lists/modules.html">Modules</a></li>
        
            
                                
            <li><a href="../../../../../lists/procedures.html">Procedures</a></li>
        
        
            <li><a href="../../../../../lists/absint.html">Abstract Interfaces</a></li>
               
            <li><a href="../../../../../lists/types.html">Derived Types</a></li>
        
        
            <li><a href="../../../../../lists/programs.html">Programs</a></li>
               
        
        
            </ul>
        
            </li>


<li class="visible-xs hidden-sm visible-lg"><a href="../../../../../lists/files.html">Source Files</a></li>



<li class="visible-xs hidden-sm visible-lg"><a href="../../../../../lists/modules.html">Modules</a></li>



<li class="visible-xs hidden-sm visible-lg"><a href="../../../../../lists/procedures.html">Procedures</a></li>


<li class="visible-xs hidden-sm visible-lg"><a href="../../../../../lists/absint.html">Abstract Interfaces</a></li>
                             
<li class="visible-xs hidden-sm visible-lg"><a href="../../../../../lists/types.html">Derived Types</a></li>


<li class="visible-xs hidden-sm visible-lg"><a href="../../../../../lists/programs.html">Programs</a></li>



          </ul>
        
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">
    
  <div class="row">
    <h1>Autotuning Framework</h1>
    <div class="row">
    <div class="col-lg-12">
    <div class="well well-sm" style="min-height: 40px;">
      <ul class="list-inline" style="margin-bottom:0px; display:inline">
         
         
<!--
        
-->
      </ul>
        <ol class="breadcrumb in-well">
      
         <li><a href='../../../../../page/index.html'>Guide</a></li>
      
         <li><a href='../../../../../page/3-developer-guide/index.html'>Developer Guide</a></li>
      
         <li><a href='../../../../../page/3-developer-guide/3-programming/index.html'>Programming</a></li>
      
         <li><a href='../../../../../page/3-developer-guide/3-programming/2-accelerator-backend/index.html'>Accelerator Backend</a></li>
      
         <li><a href='../../../../../page/3-developer-guide/3-programming/2-accelerator-backend/2-libsmm_acc/index.html'>LIBSMM (CUDA/HIP)</a></li>
      
         <li class="active">Autotuning Framework</li>
      </ol>
    </div>
    </div>
    </div>
  </div>
  
  <div class="row">
    <div class="col-md-9 col-md-push-3" id='text'>
      <h1>Auto-tuning Procedure for Finding Optimal CUDA/HIP Kernel Parameters in <code>libsmm_acc</code></h1>
<p>The performance of the matrix-matrix multiplication kernels is highly dependent on the choice of algorithm and parameters. This is why auto-tuning is used to find optimal kernel parameters.</p>
<hr>
<h3>Requirements</h3>
<p>Python version required: <code>python 3.6</code></p>
<p>If you are about to autotune parameters for a new GPU (i.e. a GPU for which there are no auto-tuned parameters yet), please first follow <a href="../README.md#adding-support-for-a-new-gpu-card">the instructions for a new GPU</a>.</p>
<p>Install all python packages required (if you do not want this project's requirements to interfere with your other Python projects, consider doing so in a <a href="https://docs.python.org/3/tutorial/venv.html">virtual environment</a>), using</p>
<div class="codehilite"><pre><span></span><code>pip install -r requirements.txt
</code></pre></div>

<hr>
<h3>Auto-tuning procedure</h3>
<h4>1. Go to the <code>libsmm_acc/tune</code> directory</h4>
<div class="codehilite"><pre><span></span><code>$ <span class="nb">cd</span> dbcsr/src/acc/libsmm_acc/tune
</code></pre></div>

<p>The <code>parameters.h</code> file (a C++ header file generated from the JSON record of multiplication kernels and their optimal parameters) is needed for the auto-tuning procedure. One can copy it over from a build directory for example, as follows:</p>
<div class="codehilite"><pre><span></span><code>$ cp ~/dbcsr/build_dir/src/acc/libsmm_acc/parameters.h ../
</code></pre></div>

<h4>2. Adapt <code>tune_setup.py</code> to your environment</h4>
<p>The <code>tune_setup.py</code> script generates job files. You have to adapt the script to the environment of your supercomputer and your personal settings.</p>
<div class="codehilite"><pre><span></span><code><span class="o">...</span>
  <span class="n">def</span> <span class="n">gen_jobfile</span><span class="p">(</span><span class="n">outdir</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>

    <span class="o">...</span>

    <span class="n">output</span> <span class="o">=</span> <span class="s2">&quot;#!/bin/bash -l</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;#SBATCH --nodes=</span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">num_nodes</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;#SBATCH --ntasks-per-core=1</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;#SBATCH --ntasks-per-node=1</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;#SBATCH --cpus-per-task=&quot;</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="si">%d</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">cpus_per_node</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;#SBATCH --time=</span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">time</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;#SBATCH --partition=normal</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;#SBATCH --constraint=gpu</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;source ${MODULESHOME}/init/sh;</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;module load daint-gpu</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;module unload PrgEnv-cray</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;module load PrgEnv-gnu</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="k">if</span> <span class="n">compiler</span> <span class="o">==</span> <span class="s2">&quot;nvcc&quot;</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;module load cudatoolkit/8.0.61_2.4.9-6.0.7.0_17.1__g899857c</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># i.e. compiler = hipcc</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;module load hip</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;module list</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;export CRAY_CUDA_MPS=1</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;cd $SLURM_SUBMIT_DIR </span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="n">output</span> <span class="o">+=</span> <span class="s2">&quot;date</span><span class="se">\n</span><span class="s2">&quot;</span>

    <span class="o">...</span>

<span class="o">...</span>
</code></pre></div>

<h4>3. Run the script <code>tune_setup.py</code></h4>
<h5>Script arguments</h5>
<p>Specify which GPU you are auto-tuning for by passing the appropriate <code>parameters_GPU.json</code> file as an argument with <code>-p/--params</code>.</p>
<p>Specify which compiler to use for compiling kernel code by passing <code>nvcc</code> or <code>hipcc</code> as an argument with <code>-b/--compiler</code>.</p>
<p>More arguments can be set, run</p>
<div class="codehilite"><pre><span></span><code>$ ./tune_setup.py --help
</code></pre></div>

<p>for more information.</p>
<h5>Script positional arguments (block sizes to autotune)</h5>
<p>In addition, the script takes as arguments the block sizes you want to add to <code>libsmm_acc</code>. You can specify these as a list of integers or provide the parameter file of a different GPU from which to read the block sizes to autotune.</p>
<h5>Examples</h5>
<p>For example, if the system you want to autotune for contains blocks of size 5 and 8, run:</p>
<div class="codehilite"><pre><span></span><code>$ ./tune_setup.py <span class="m">5</span> <span class="m">8</span> -p ../parameters/parameters_P100.json
Reading parameters from parameters_P100.json
libsmm_acc: Found <span class="m">74096</span> existing parameter sets, of which <span class="m">1641</span> are autotuned and <span class="m">72455</span> are predicted.
Requested to autotune <span class="m">8</span> triplets
Found <span class="m">41824</span> parameter sets <span class="k">for</span> 5x5x5
Found <span class="m">83648</span> parameter sets <span class="k">for</span> 5x5x8
Found <span class="m">103072</span> parameter sets <span class="k">for</span> 5x8x5
Found <span class="m">103072</span> parameter sets <span class="k">for</span> 5x8x8
Found <span class="m">103072</span> parameter sets <span class="k">for</span> 8x5x5
Found <span class="m">103072</span> parameter sets <span class="k">for</span> 8x5x8
Found <span class="m">125344</span> parameter sets <span class="k">for</span> 8x8x5
Found <span class="m">125344</span> parameter sets <span class="k">for</span> 8x8x8
</code></pre></div>

<p>Or, if you want to obtain, for the NVIDIA P100, the parameters of the same block sizes as recorded for the NVIDIA K40, run:</p>
<div class="codehilite"><pre><span></span><code>$ ./tune_setup.py -p ../parameters/parameters_P100.json ../parameters/parameters_K40.json
Reading parameters from parameters_P100.json
libsmm_acc: Found <span class="m">74093</span> existing parameter sets, of which <span class="m">1638</span> are autotuned and <span class="m">72455</span> are predicted.
Reading parameters to autotune from parameters_K40.json
Requested to autotune <span class="m">19</span> triplets
Found <span class="m">41824</span> parameter sets <span class="k">for</span> 5x5x5
Found <span class="m">95648</span> parameter sets <span class="k">for</span> 6x6x6
Found <span class="m">110496</span> parameter sets <span class="k">for</span> 7x7x7
Found <span class="m">125344</span> parameter sets <span class="k">for</span> 8x8x8
Found <span class="m">173764</span> parameter sets <span class="k">for</span> 9x9x9
...
</code></pre></div>

<h5>Output</h5>
<p>The script will create a directory for each combination of the block sizes:</p>
<div class="codehilite"><pre><span></span><code>$ ls -d tune_*
tune_5x5x5  tune_5x5x8  tune_5x8x5  tune_5x8x8  tune_8x5x5  tune_8x5x8  tune_8x8x5  tune_8x8x8
</code></pre></div>

<p>Each directory contains a number of files:</p>
<div class="codehilite"><pre><span></span><code>$ ls -1 tune_8x8x8/
Makefile
tune_8x8x8_exe0_main.cu/cpp
tune_8x8x8_exe0_part0.cu/cpp
tune_8x8x8_exe0_part1.cu/cpp
tune_8x8x8_exe0_part2.cu/cpp
tune_8x8x8_exe0_part3.cu/cpp
tune_8x8x8_exe0_part4.cu/cpp
tune_8x8x8.job
</code></pre></div>

<p>For each possible parameter-set a <em>launcher</em> is generated. A launcher is a small snippet of C code, which launches the kernel by using the CUDA specific <code>&lt;&lt;&lt; &gt;&gt;&gt;</code>-notation or HIP's <code>hipLaunchKernelGGL</code> function. It also instantiates the C++ template which contains the actual kernel code.</p>
<p>In order to parallelize the benchmarking, the launchers are distributed over multiple executables. Currently, up to 10'000 launchers are benchmarked by one <em>executable</em>. Each executable is linked together from several <code>tune_*_part???.o</code> and a <code>tune_*_main.o</code>. Each part-files contains up to 100 launchers. This allows to parallelize the compilation over multiple CPU cores.</p>
<h4>4. Adapt <code>tune_submit.py</code> to your environment</h4>
<p>The script <code>tune_submit.py</code> was written for the slurm batch system as used e.g. by CRAY supercomputers. If your computer runs a different batch system, you have to adapt <code>tune_submit.py</code> accordingly.</p>
<h4>5. Submit Jobs</h4>
<p>Each tune-directory contains a job file. Since there might be many tune-directories, the convenience script <code>tune_submit.py</code> can be used to submit jobs. It will go through all the <code>tune_*</code>-directories and check if its job has already been submitted or run. For this, the script calls <code>squeue</code> in the background and it searches for <code>slurm-*.out</code>files. In order to limit the number of jobs submitted at a time, a maximum number of jobs to submit can be specified with <code>-j</code>.</p>
<p>When <code>tune_submit.py</code> is called without arguments, it will just list the jobs that could be submitted:</p>
<div class="codehilite"><pre><span></span><code>$ ./tune_submit.py 
          tune_5x5x5: Would submit, run with <span class="s2">&quot;doit!&quot;</span>
          tune_5x5x8: Would submit, run with <span class="s2">&quot;doit!&quot;</span>
          tune_5x8x5: Would submit, run with <span class="s2">&quot;doit!&quot;</span>
          tune_5x8x8: Would submit, run with <span class="s2">&quot;doit!&quot;</span>
          tune_8x5x5: Would submit, run with <span class="s2">&quot;doit!&quot;</span>
          tune_8x5x8: Would submit, run with <span class="s2">&quot;doit!&quot;</span>
          tune_8x8x5: Would submit, run with <span class="s2">&quot;doit!&quot;</span>
          tune_8x8x8: Would submit, run with <span class="s2">&quot;doit!&quot;</span>
Number of <span class="nb">jobs</span> submitted: <span class="m">8</span>
</code></pre></div>

<p>Only when <code>tune_submit.py</code> is called with <code>doit!</code> as its first argument, will it actually submit jobs:</p>
<div class="codehilite"><pre><span></span><code>$ ./tune_submit.py doit!
          tune_5x5x5: Submitting
Submitted batch job <span class="m">277987</span>
          tune_5x5x8: Submitting
Submitted batch job <span class="m">277988</span>
          tune_5x8x5: Submitting
Submitted batch job <span class="m">277989</span>
          tune_5x8x8: Submitting
Submitted batch job <span class="m">277990</span>
          tune_8x5x5: Submitting
Submitted batch job <span class="m">277991</span>
          tune_8x5x8: Submitting
Submitted batch job <span class="m">277992</span>
          tune_8x8x5: Submitting
Submitted batch job <span class="m">277993</span>
          tune_8x8x8: Submitting
Submitted batch job <span class="m">277994</span>
Number of <span class="nb">jobs</span> submitted: <span class="m">8</span>
</code></pre></div>

<h4>6. Collect Results</h4>
<p>Run <code>tune_collect.py</code> to parse all log files and determine the best kernel for each blocksize:</p>
<div class="codehilite"><pre><span></span><code>$ ./tune_collect.py
Reading: tune_5x5x5/tune_5x5x5_exe0.log
Reading: tune_5x5x8/tune_5x5x8_exe0.log
Reading: tune_5x8x5/tune_5x8x5_exe0.log
Reading: tune_5x8x8/tune_5x8x8_exe0.log
Reading: tune_8x5x5/tune_8x5x5_exe0.log
Reading: tune_8x5x8/tune_8x5x8_exe0.log
Reading: tune_8x8x5/tune_8x8x5_exe0.log
Reading: tune_8x8x8/tune_8x8x8_exe0.log
Kernel_dnt_tiny<span class="o">(</span><span class="nv">m</span><span class="o">=</span><span class="m">5</span>, <span class="nv">n</span><span class="o">=</span><span class="m">5</span>, <span class="nv">k</span><span class="o">=</span><span class="m">5</span>, <span class="nv">split_thread</span><span class="o">=</span><span class="m">32</span>, <span class="nv">threads</span><span class="o">=</span><span class="m">64</span>, <span class="nv">grouping</span><span class="o">=</span><span class="m">16</span>, <span class="nv">minblocks</span><span class="o">=</span><span class="m">1</span><span class="o">)</span> , <span class="c1"># 27.9623 GFlops</span>
Kernel_dnt_tiny<span class="o">(</span><span class="nv">m</span><span class="o">=</span><span class="m">5</span>, <span class="nv">n</span><span class="o">=</span><span class="m">5</span>, <span class="nv">k</span><span class="o">=</span><span class="m">8</span>, <span class="nv">split_thread</span><span class="o">=</span><span class="m">32</span>, <span class="nv">threads</span><span class="o">=</span><span class="m">96</span>, <span class="nv">grouping</span><span class="o">=</span><span class="m">16</span>, <span class="nv">minblocks</span><span class="o">=</span><span class="m">1</span><span class="o">)</span> , <span class="c1"># 37.8978 GFlops</span>
Kernel_dnt_medium<span class="o">(</span><span class="nv">m</span><span class="o">=</span><span class="m">5</span>, <span class="nv">n</span><span class="o">=</span><span class="m">8</span>, <span class="nv">k</span><span class="o">=</span><span class="m">5</span>, <span class="nv">tile_m</span><span class="o">=</span><span class="m">1</span>, <span class="nv">tile_n</span><span class="o">=</span><span class="m">1</span>, <span class="nv">threads</span><span class="o">=</span><span class="m">96</span>, <span class="nv">grouping</span><span class="o">=</span><span class="m">16</span>, <span class="nv">minblocks</span><span class="o">=</span><span class="m">8</span><span class="o">)</span> , <span class="c1"># 32.9231 GFlops</span>
Kernel_dnt_tiny<span class="o">(</span><span class="nv">m</span><span class="o">=</span><span class="m">5</span>, <span class="nv">n</span><span class="o">=</span><span class="m">8</span>, <span class="nv">k</span><span class="o">=</span><span class="m">8</span>, <span class="nv">split_thread</span><span class="o">=</span><span class="m">32</span>, <span class="nv">threads</span><span class="o">=</span><span class="m">96</span>, <span class="nv">grouping</span><span class="o">=</span><span class="m">16</span>, <span class="nv">minblocks</span><span class="o">=</span><span class="m">1</span><span class="o">)</span> , <span class="c1"># 47.0366 GFlops</span>
Kernel_dnt_medium<span class="o">(</span><span class="nv">m</span><span class="o">=</span><span class="m">8</span>, <span class="nv">n</span><span class="o">=</span><span class="m">5</span>, <span class="nv">k</span><span class="o">=</span><span class="m">5</span>, <span class="nv">tile_m</span><span class="o">=</span><span class="m">1</span>, <span class="nv">tile_n</span><span class="o">=</span><span class="m">1</span>, <span class="nv">threads</span><span class="o">=</span><span class="m">96</span>, <span class="nv">grouping</span><span class="o">=</span><span class="m">16</span>, <span class="nv">minblocks</span><span class="o">=</span><span class="m">12</span><span class="o">)</span> , <span class="c1"># 33.1999 GFlops</span>
Kernel_dnt_medium<span class="o">(</span><span class="nv">m</span><span class="o">=</span><span class="m">8</span>, <span class="nv">n</span><span class="o">=</span><span class="m">5</span>, <span class="nv">k</span><span class="o">=</span><span class="m">8</span>, <span class="nv">tile_m</span><span class="o">=</span><span class="m">1</span>, <span class="nv">tile_n</span><span class="o">=</span><span class="m">1</span>, <span class="nv">threads</span><span class="o">=</span><span class="m">96</span>, <span class="nv">grouping</span><span class="o">=</span><span class="m">16</span>, <span class="nv">minblocks</span><span class="o">=</span><span class="m">12</span><span class="o">)</span> , <span class="c1"># 49.3499 GFlops</span>
Kernel_dnt_tiny<span class="o">(</span><span class="nv">m</span><span class="o">=</span><span class="m">8</span>, <span class="nv">n</span><span class="o">=</span><span class="m">8</span>, <span class="nv">k</span><span class="o">=</span><span class="m">5</span>, <span class="nv">split_thread</span><span class="o">=</span><span class="m">32</span>, <span class="nv">threads</span><span class="o">=</span><span class="m">96</span>, <span class="nv">grouping</span><span class="o">=</span><span class="m">16</span>, <span class="nv">minblocks</span><span class="o">=</span><span class="m">1</span><span class="o">)</span> , <span class="c1"># 62.8469 GFlops</span>
Kernel_dnt_tiny<span class="o">(</span><span class="nv">m</span><span class="o">=</span><span class="m">8</span>, <span class="nv">n</span><span class="o">=</span><span class="m">8</span>, <span class="nv">k</span><span class="o">=</span><span class="m">8</span>, <span class="nv">split_thread</span><span class="o">=</span><span class="m">32</span>, <span class="nv">threads</span><span class="o">=</span><span class="m">128</span>, <span class="nv">grouping</span><span class="o">=</span><span class="m">16</span>, <span class="nv">minblocks</span><span class="o">=</span><span class="m">1</span><span class="o">)</span> , <span class="c1"># 90.7763 GFlops</span>

Wrote parameters.json
</code></pre></div>

<p>The file <code>parameters.json</code> in <code>dbcsr/src/acc/libsmm_acc/parameters</code> now contains the newly autotuned parameters.</p>
<h4>7. Merge new parameters with original parameter-file</h4>
<p>Run <code>tune_merge.py</code> to merge the new parameters with the original ones:</p>
<div class="codehilite"><pre><span></span><code>$ ./tune_merge.py
Merging parameters.json with parameters_P100.json
Wrote parameters.new.json
</code></pre></div>

<p>The file <code>parameters.new.json</code> can now be used as a parameter file. Rename it to <code>parameters_GPU.json</code>, with the appropriate <code>GPU</code>.</p>
<h4>8. (optional) Explore the data</h4>
<p>Explore the data interactively using the <a href="notebooks/inspect_training_data.ipynb">provided Jupyter Notebook</a>.</p>
<h4>9. Contribute parameters to the community</h4>
<p><strong>Contribute new optimal parameters</strong></p>
<p>Submit a pull request updating the appropriate <code>parameters_GPU.json</code> file to the <a href="https://github.com/cp2k/dbcsr">DBCSR repository</a>.</p>
<p><strong>Contribute autotuning data</strong></p>
<p>See <a href="https://github.com/cp2k/dbcsr-data#contributing">instructions</a> in DBCSR's <a href="https://github.com/cp2k/dbcsr-data">data repository</a>.</p>
    </div>
    
    <div class="col-md-3 col-md-pull-9">
      <hr class="visible-xs visible-sm">
        <div class="well toc">
          <ul class="nav nav-stacked nav-pills">
            <li role="presentation" class="title"><a href='../../../../../page/index.html'>Guide</a></li>
          </ul>
          <hr>
          <ul class="nav nav-stacked nav-pills">
            
            <li role="presentation">
            <a href='../../../../../page/1-DBCSR/index.html'>DBCSR</a>
            
            <ul class="nav nav-stacked nav-pills">
              
            <li role="presentation">
            <a href='../../../../../page/1-DBCSR/publications.html'>Publications</a>
            
            </li>
            
            </ul>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/2-user-guide/index.html'>User Guide</a>
            
            <ul class="nav nav-stacked nav-pills">
              
            <li role="presentation">
            <a href='../../../../../page/2-user-guide/1-installation/index.html'>Install</a>
            
            <ul class="nav nav-stacked nav-pills">
              
            <li role="presentation">
            <a href='../../../../../page/2-user-guide/1-installation/1-cmake-build-recipes.html'>CMake Build Recipes</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/2-user-guide/1-installation/2-supported-compilers.html'>Supported Compilers</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/2-user-guide/1-installation/3-using-dbcsr-in-a-cmake-project.html'>Using DBCSR in a CMake project</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/2-user-guide/1-installation/4-docker.html'>Docker Images</a>
            
            </li>
            
            </ul>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/2-user-guide/2-tests/index.html'>Tests</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/2-user-guide/3-examples/index.html'>Examples</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/2-user-guide/4-gpu/index.html'>GPUs</a>
            
            </li>
            
            </ul>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/index.html'>Developer Guide</a>
            
            <ul class="nav nav-stacked nav-pills">
              
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/1-tooling/index.html'>Tooling</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/2-documentation/index.html'>Documentation</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/3-programming/index.html'>Programming</a>
            
            <ul class="nav nav-stacked nav-pills">
              
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/3-programming/1-overview/index.html'>Overview</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/3-programming/2-accelerator-backend/index.html'>Accelerator Backend</a>
            
            <ul class="nav nav-stacked nav-pills">
              
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/3-programming/2-accelerator-backend/1-code-structure.html'>Code Structure</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/3-programming/2-accelerator-backend/2-libsmm_acc/index.html'>LIBSMM (CUDA/HIP)</a>
            
            <ul class="nav nav-stacked nav-pills">
              
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/3-programming/2-accelerator-backend/2-libsmm_acc/1-kernels.html'>Kernels</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/3-programming/2-accelerator-backend/2-libsmm_acc/2-parameters.html'>Kernel Parameters</a>
            
            </li>
            
            <li role="presentation" class="disabled">
            <a href='../../../../../page/3-developer-guide/3-programming/2-accelerator-backend/2-libsmm_acc/3-tune.html'>Autotuning Framework</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/3-programming/2-accelerator-backend/2-libsmm_acc/4-predict.html'>Predictive Modeling Framework</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/3-programming/2-accelerator-backend/2-libsmm_acc/5-notebooks.html'>Notebooks</a>
            
            </li>
            
            </ul>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/3-programming/2-accelerator-backend/3-opencl-backend.html'>OpenCL Backend</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/3-programming/2-accelerator-backend/4-opencl-libsmm.html'>OpenCL LIBSMM</a>
            
            </li>
            
            </ul>
            
            </li>
            
            </ul>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/4-performance/index.html'>Performance</a>
            
            <ul class="nav nav-stacked nav-pills">
              
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/4-performance/1-insights.html'>Insights</a>
            
            </li>
            
            <li role="presentation">
            <a href='../../../../../page/3-developer-guide/4-performance/2-just-in-time-compilation.html'>JIT</a>
            
            </li>
            
            </ul>
            
            </li>
            
            </ul>
            
            </li>
            
          </ul>
        </div>
    </div>
    
  </div>

    <hr>    
    </div> <!-- /container -->
    <footer>
      <div class="container">
      <div class="row">
        <div class="col-xs-6 col-md-6"><p>DBCSR was developed by DBCSR Authors<br>&copy; 2021 
                                          </p>
        </div>
        <div class="col-xs-6 col-md-6">
          <p class="text-right">
            Documentation generated by 
            <a href="https://github.com/Fortran-FOSS-Programmers/ford">FORD</a>
            
            
          </p>
        </div>
      </div>
      <br>
      </div> <!-- /container -->    
    </footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
<!--
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
-->
    <script src="../../../../../js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../../../../js/ie10-viewport-bug-workaround.js"></script>

    <!-- MathJax JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },
        jax: ['input/TeX','input/MathML','output/HTML-CSS'],
        extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']
      });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    
    
  </body>
</html>